---
title: "Homework 5 - Answers"
output: html_document
---

```{r, message=FALSE, warning=FALSE}
library(Lahman)
library(tidyverse)
library(broom)
```

# Problem 1 - Money Ball

_Moneyball: The Art of Winning an Unfair Game_ is a book by Michael Lewis about the Oakland Athletics baseball team in 2002 and its general manager, the person tasked with building the team, Billy Beane. During Billy Bean's tenure as general manager, ownership cut the budget drastically leaving the general manager with one of the lowest payrolls in baseball. Money Ball tells the story of how Billy Bean used analysts to find inefficiencies in the market. Specifically, his team used data science to find low cost players that the data predicted would help the team win.

Statistics have been used in baseball since its beginnings. Note that `Lahman` (a library containing an extensive baseball database) goes back to the 19th century. Batting average, for example, has been used to summarize a batter's success for decades. [Other statistics](http://mlb.mlb.com/stats/league_leaders.jsp) such as home runs (HR), runs batted in (RBI) and stolen bases have been reported and players rewarded for high numbers. However, until [Bill James](https://en.wikipedia.org/wiki/Bill_James) introduced [sabermetrics](https://en.wikipedia.org/wiki/Sabermetrics), careful analyses had not been done to determine if these statistics actually help a team win. To simplify the exercise we will focus on scoring runs and ignore pitching and fielding. 

## Problem 1A

Here, we will use the `Lahman` library. You can see tables that are available when you load this package by typing:

```{r, eval=FALSE}
?Lahman
```

Use the data in the `Teams` table to explore the relationship between stolen bases (SB) and runs per game in 1999. Make a plot, fit a regression line, and report the coefficients. If you take the coefficient at face value, how many more runs per game does a team score for every extra SB per game?

**Solution:**

```{r}
## put your code here
theme_set(theme_bw())
fit <- Teams %>%
  filter(yearID == 1999) %>%
  mutate(R = R / G, SB = SB / G) %>%
  lm(R ~ SB, data = .) 

Teams %>%
  filter(yearID == 1999) %>%
  mutate(R = R / G, SB = SB / G) %>% 
  ggplot(aes(SB, R)) + 
  ylab("R per game")+
  xlab("SB per game")+
  geom_point() +
  geom_abline(intercept = fit$coef[1],
              slope = fit$coef[2])

coef(fit)
```

Taking the coefficients at face value, a team will earn on average `r signif(coef(fit)[2],2)` more runs per game for every SB.

## Problem 1B

In Problem 1A we observed a positive relationship between scoring runs and stealing bases. However, the estimated slope coefficient is a random variable. There is chance involved in scoring a run. So how do we know if this observed relationship was not just chance variability?

To examine the variability of this random variable we will consider each year to be a new independent outcome. Use the `lm` and `do` functions to fit a linear model to each year since 1961 (when they started playing 162 games per year). Hint: use the function `tidy` in `broom` to process the regression in each group so that it can be recombined (see [here](https://cran.r-project.org/web/packages/broom/vignettes/broom_and_dplyr.html) for examples).

Using this approach, what is your estimate of the slope random variable's standard error? Is the distribution of the random variable well approximated by a normal distribution? If so, use this to provide a 95% confidence interval for our effect of stolen bases on runs per game. Do you think stolen bases help score runs?

**Solution:**

```{r}
## put your code here
res <- Teams %>% filter(yearID >= 1961) %>%
  mutate(R = R / G, SB = SB / G) %>%
  group_by(yearID) %>%
  do(tidy(lm(R ~ SB, data = .))) %>%
  filter(term == "SB")

### ThE SE is:
sd(res$estimate)
##The CI is
mean(res$estimate) + c(-1,1)*qnorm(0.975)*sd(res$estimate)
qqnorm(res$estimate)
qqline(res$estimate)
```

The estimates are approximately normally distributed (as shown in the qqplot). The 95% CI covers zero, indicating that increase in SB does not significantly help scoring runs.

Students may also examine a histogram of the coefficient estimates to evaluate normality.

## Problem 1C

Even if we didn't have several years to examine the distribution of our estimate, there is a version of the CLT that applies to regression. It turns out that with a large enough sample size, in this case the number of teams, we can construct a confidence interval. Use the function `tidy` to report a confidence interval for the effect of SB on runs based exclusively on the 1999 data. What are your thoughts now on the effectiveness of recruiting players that can steal bases?

**Solution:**

```{r}
## put your code here
res <- tidy(fit, conf.int = TRUE) # `fit` is from 1A
res %>%
  filter(term == "SB")

# or
res <- summary(fit)
res$coef[2,1] + c(-1,1)*qnorm(0.975)*res$coef[2,2]
```

It seems not to be effective to recruit players that can steal bases as it doesn't significantly increase the runs scored.

## Problem 1D

Back in 2002 (the year of the [money ball](https://en.wikipedia.org/wiki/Moneyball) story described above), bases on balls (BB) did not receive as much attention as other statistics. Repeat the above analysis we performed in 1C for BB per game in 1999. Do BB have a larger effect on runs than SB?

**Solution:**

```{r}
## put your code here
fit1 <- Teams %>%
  filter(yearID == 1999) %>%
  mutate(R = R / G, BB = BB / G) %>%
  lm(R ~ BB, data = .)

res <- tidy(fit1, conf.int = TRUE)
res %>%
  filter(term == "BB")

# or
res <- summary(fit1)
res$coef[2,1] + c(-1,1)*qnorm(0.975)*res$coef[2,2]
```

Yes. BB do have a larger effect on runs than SB. On average there are `r signif(res$coef[2,1], 2)` more runs per game for every extra BB. The confidence interval does not include zero, so this difference is statistically significant.

## Problem 1E

Association is not causation. It turns out that HR hitters also obtain many BB. We know for a fact that HRs cause runs because, by definition, they produce at least one. We can see this by simply plotting these two statistics for all players with more than 500 plate appearances (`BB+AB`):

```{r}
Batting %>%
  filter(yearID >= 1961 & BB+AB > 500 & !is.na(HR) & !is.na(BB)) %>% 
  mutate(HR = factor(pmin(HR, 40))) %>%
  ggplot(aes(HR, BB)) +
  geom_boxplot()
```

So, is the relationship we saw above for BB and runs due to teams having more HRs also having more BBs? One way we can explore this is by keeping HR fixed and examining the relationship within the strata. For example, if we look only at teams with 150 HRs, do more BBs produce more runs?

We can't perform this analysis on a single year, because there are not enough teams to obtain strata with more than one or two teams. Instead we will combine all data across years since 1961. 

Group data by the number of HRs and perform a regression analysis in each stratum to determine the effect of BB per game on runs per game. Use 10th, 20th, ... quantiles to split the data into 10 groups. Hint: use the function `cut` and `quantile` to create the strata. Does the relationship between BB and runs seem linear within each strata?

**Solution:**

```{r}
## put your code here
my_data <- Teams %>%
  filter(yearID >= 1961) %>% 
  mutate(R = R / G, BB = BB / G, HR = HR / G) %>% 
  mutate(group = cut(HR, quantile(HR, prob = seq(0, 1, .1)), include.lowest = TRUE))

res <- my_data %>%
  group_by(group) %>%
  do(tidy(lm(R ~ BB, data = .))) %>%
  filter(term == "BB")

res
## Visual inspection seems to show the relationship is linear in each strate
my_data %>%
  ggplot(aes(BB, R)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~group)
```

## Problem 1F

In problem 1E, we saw that the effect of BB on runs appears to be about the same in each strata. The relationship between HR and R is also, not surprisingly, linear:

```{r}
Teams %>%
  filter(yearID >= 1961) %>% 
  mutate(R = R / G, HR = HR / G) %>%
  ggplot(aes(HR, R)) +
  geom_point()
```

These two combined implies that a sensible linear model says:

$$
\mbox{Runs} = \beta_0 + \beta_{BB} \mbox{BB} + \beta_{HR}{HR} + \varepsilon
$$

In this model, we _adjust_ for HRs by including it as linear term. Note that we have already shown data that support this model. In general, simply fitting such a model does not necessarily adjust for a possible confounder. The model must also be approximately correct.

We can fit this model like this:

```{r}
fit <- Teams %>%
  filter(yearID >= 1961) %>% 
  mutate(R = R / G, BB = BB / G, HR = HR / G) %>%
  lm(R ~ BB + HR, data = .)
summary(fit)
```

Note that the summary shows a very strong HR effect but also a decent BB effect. Now, what happens if we include singles (`H-X2B-X3B-HR`), extra bases (doubles plus triples, `X2B + X3B`), and HRs per game in our model? What does the model say about which of these characteristics should receive more weight? 

Also, fit the model to each year independently to check for consistency from year to year. Does the model appear consistent over time?

**Solution:**

```{r}
## put your code here
fit <- Teams %>% filter(yearID>=1961) %>%  
  mutate( R = R / G, BB = BB / G, 
          Singles = (H - X2B - X3B - HR) / G,
          XB = (X2B + X3B) / G, HR = HR / G) %>%
  lm(R ~ BB + Singles + XB + HR, data = .)
summary(fit)
```

This model still shows a very strong HR effect, and the BB effect is slightly diminished. The singles and XB effects are intermediate.

```{r}
### You may want to check for consistency by year
Teams %>%
  filter(yearID >= 1961) %>%
  group_by(yearID) %>%
  mutate(R = R / G, BB = BB / G, Singles = (H - X2B - X3B - HR) / G,
         XB = (X2B + X3B) / G, HR = HR / G) %>%
  do(tidy(lm(R ~ BB + Singles + XB + HR, data = .))) %>%
  filter(!grepl("Intercept", term)) %>%
  ggplot(aes(yearID, estimate, group = term, col = term)) +
  geom_line() + 
  geom_point()
```

While we see some varability in the coefficient estimates from year to year, the ordering (HR being strongest and BB being weakest) is consistent over time.

# Problem 2 - Reporting Heights Correctly?

Load the `heights` data from the dslabs package:

```{r}
library(dslabs)
data("heights")
```

Note that there are some very low heights reported. Here are the three shortest females:

```{r}
heights %>% 
  filter(sex == "Female") %>% 
  arrange(height) %>% 
  slice(1:3)
```

To quantify how below average these heights are, let's assume female heights follow a normal distribution and compute the probability of picking a female that is 55 inches or shorter. 

## Problem 2A

Start by computing the average and the standard deviation for female heights.

**Solution:**

```{r}
## Your code here
summaries <- heights %>% 
  filter(sex == 'Female') %>% 
  summarise(average = mean(height),
            sd = sd(height))

avg <- summaries$average
sd <- summaries$sd
```

## Problem 2B

If we approximate the female heights with a normal distribution with the average and standard deviation you just computed, what is the probability of picking a person at random and seeing someone 55 inches or shorter?

**Solution:**

```{r}
## Your code here
prb <- pnorm(55, avg, sd)
prb
```

Assuming female heights approximately follow a normal distribution, the probability of picking someone 55 inches or shorter is `r signif(prb,3)`.

## Problem 2C

The probability is quite low. Is it possible that the three shortest people meant to enter 5'5 (65 inches) and forgot to enter the `'`? 

We will use a Bayesian analysis to answer this. Suppose we pick a female at random. Let the random variable representing the actual height be $X$ and the reported height $Y$. Let $Z$ be a random variable for whether the person entered their height incorrectly and forgot the `'`. $Z=1$ if they made a mistake. 

So if the reported height is $Y=55$ and $Z=1$ the actual height is $X=5*12+5=65$, and if $Z=0$ then the actual and reported height are the same $X=Y=55$. 

Assume that the probability of making this error is $\mbox{Pr}(Z=1)$ = $\pi$. Use Bayes' formula to compute the probability they made a mistake given that $55$ is reported. Express the following probability as a function of $\pi$, then compute it for $\pi = 0.01$.

$$\mbox{Pr}(Z = 1 \mid Y=55)$$

**Solution:**

$$
\mbox{Pr}(Z = 1 \mid Y=55) =
\frac{ \mbox{Pr}( Y =55 | Z =1 ) \pi}
{\mbox{Pr}(Y=55|Z=1) \pi + \mbox{Pr}(Y=55|Z=0) (1-\pi)}
$$
Now if $f_X$ is the normal density, for the actual density this is equal to:

$$
\mbox{Pr}(Z = 1 \mid Y=55) =
\frac{ f_X(65) \pi}
{f_X(65) \pi + f_X(55) (1-\pi)}
$$

We can compute the probability that an error was made given a reported height of 55 (assuming heights in our dataset are approximately normal) as:

```{r}
## Your code here
pi <- 0.01
dnorm(65, avg, sd) * pi/
  (dnorm(65, avg, sd) * pi + dnorm(55, avg, sd) * (1-pi))
```

## Problem 2D

Now, what if we look at different $\pi$ values? Generate a plot for $\mbox{Pr}(Z = 1 \mid Y=55)$ for `\pi` from 0.001 to 0.10. What do you observe?

**Solution:**

```{r}
## Your code here
pi <- seq(0.001, 0.10, len=100) 
posterior <- dnorm(65, avg, sd) * pi/
  (dnorm(65, avg, sd) * pi + dnorm(55, avg, sd) * (1-pi))
  
data.frame(posterior,pi) %>% 
  ggplot() +
  geom_point(aes(x = pi, y = posterior))+
  ylab("Probability of making error | reported height as 55''")
```

As $\pi$ (probability of making reporting error) increases, the probability that a reported height as 55'' is a reporting error increases.

## Problem 2E

What if we look at different heights ($y$)? Generate a plot for $\mbox{Pr}(Z = 1 \mid Y = y)$ for $y=50,\dots,59$ when $\pi$ = 0.01. What do you think about this pattern? Note: recalling from 2C that: 

$$
\mbox{Pr}(Z = 1 \mid Y=y) =
\frac{ f_X(y+10) \pi}
{f_X(y+10) \pi + f_X(y) (1-\pi)}
$$

**Solution:**

```{r}
## Your code here
pi <- 0.01
y <- seq(50, 59, 1)
posterior <- dnorm(y+10, avg, sd) * pi/
  (dnorm(y+10, avg, sd) * pi + dnorm(y, avg, sd) * (1-pi))
data.frame(posterior,y) %>%
  ggplot() +
  geom_point(aes(x = y, y = posterior))+
  ylim(0,1) +
  ylab("Probability of making error | reported height")+
  xlab("Reported height (inches)")
```

Holding $\pi$ as 0.01, for reported height between 50'' to 59'': The larger the reported height, the less likely the probability that a reported height is a reporting error.
